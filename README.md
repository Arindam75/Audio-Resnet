# Audio-Resnet
Audio classification has been mostly limited to Speech and Music domains. Interest in applying machine learning algorithms to sound produced in a typical environment started appearing only in the recent decade. While there has been a lot of research in audio feature extraction, the unavailability of annotated audio datasets has impeded research in this domain, 
until very recently. Labeled datasets like UrbanSound8K, ESC-10, and ESC-50, containing environmental audio clips are now available so that models can be benchmarked. On the other hand, Deep Convolutional Neural Network (CNN) has been steadily improving the accuracy of Computer Vision Tasks. This is made possible, partly due to a large repository of visual datasets like ImageNet, CIFAR-10, MNIST, etc. These can be used to benchmark architectures against each other. Audio Classification is fairly new to CNNs and accuracies of sequential models with a single path have been significantly lower than computer vision counterparts. CNN approach took a departure from sequential to branched or residual architectures leading to significant improvement in computer vision accuracies. In case of environmental sound classification, most CNN architectures have been sequential. Since sequential CNNs suffer from vanishing gradient as they get deeper, we build a deep non-sequential architecture like ResNet to verify if we can improve the classification accuracy of the ESC-10 dataset.<br>The accuracy results we obtained from various models indicate the positive impact of introducing residual connections. We present a comparative analysis of the models in project and also future studies that can be undertaken based on the findings of this report.
As we increased the depth of the sequential models, we noticed that the validation accuracy peaked and then fell. We were able to recover most of the lost performance by introducing the skip connection
